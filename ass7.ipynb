{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2514b52-8f3e-4cdb-aeba-2841dd590b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Text Analytics \\n1. Extract Sample document and apply following document preprocessing methods: \\nTokenization, POS Tagging, stop words removal, Stemming and Lemmatization. \\n2. Create representation of document by calculating Term Frequency and Inverse Document \\nFrequency. \\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " Text Analytics \n",
    "1. Extract Sample document and apply following document preprocessing methods: \n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization. \n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document \n",
    "Frequency. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cec3072e-530b-421e-abf0-e065db48ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.25.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c759d7e9-3bae-46da-b150-a67876e63828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‭Welcome to Smallpdf‬\n",
      "‭Ready to take document management to the next level?‬\n",
      "‭Digital Documents—All In One Place‬\n",
      "‭With the new Smallpdf experience, you can‬\n",
      "‭freely upload, organize, and share digital‬\n",
      "‭documents. When you enable the‬‭‘Storage’‬\n",
      "‭option‬‭, we’ll also store all processed files here.‬\n",
      "‭Enhance Documents in One Click‬\n",
      "‭When you right-click on a file, we’ll present‬\n",
      "‭you with an array of options to convert,‬\n",
      "‭compress, or modify it.‬\n",
      "‭Access Files Anytime, Anywhere‬\n",
      "‭You can access files stored on Smallpdf from‬\n",
      "‭your computer, phone, or tablet. We’ll also‬\n",
      "‭sync files from the‬‭Smallpdf Mobile App‬‭to our‬\n",
      "‭online portal‬\n",
      "‭Collaborate With Others‬\n",
      "‭Forget mundane administrative tasks. With‬\n",
      "‭Smallpdf, you can request e-signatures, send‬\n",
      "‭large files, or even enable the‬‭Smallpdf G Suite‬\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "print(fitz.open(r\"D:\\sample.pdf\").get_page_text(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2373ede6-db47-433a-9067-b54a10f69f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c25bb3bb-15d3-4435-a029-dfa0893b18c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")  # Sentence and word tokenizer\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5842a51d-ba20-4d50-8d8b-696186580f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sachin', 'was', 'the', 'GOAT', 'of', 'the', 'previous', 'generation', '.', 'Virat', 'is', 'the', 'GOAT', 'of', 'this', 'generation', '.', 'Shubman', 'will', 'be', 'the', 'GOAT', 'of', 'the', 'next', 'generation']\n",
      "['Sachin was the GOAT of the previous generation.', 'Virat is the GOAT of this generation.', 'Shubman will be the GOAT of the next generation']\n"
     ]
    }
   ],
   "source": [
    "#tokenization\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "corpus = \"Sachin was the GOAT of the previous generation. Virat is the GOAT of this generation. Shubman will be the GOAT of the next generation\"\n",
    "print(nltk.word_tokenize(corpus))\n",
    "print(nltk.sent_tokenize(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8e64c11-e76e-4280-9bcd-2ae962bd057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sachin', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('GOAT', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('previous', 'JJ'), ('generation', 'NN'), ('.', '.'), ('Virat', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('GOAT', 'NNP'), ('of', 'IN'), ('this', 'DT'), ('generation', 'NN'), ('.', '.'), ('Shubman', 'NNP'), ('will', 'MD'), ('be', 'VB'), ('the', 'DT'), ('GOAT', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('next', 'JJ'), ('generation', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Part of speech TAGGING\n",
    "from nltk import pos_tag\n",
    "tokens = word_tokenize(corpus)\n",
    "print(pos_tag(tokens))\n",
    "#Tag\tMeaning\t                       Example\n",
    "#NNP\tProper noun, singular\t    India, Python, John\n",
    "#VBD\tVerb, past tense\t         ate, went, played\n",
    "#DT\t    Determiner\t                 the, a, an, this\n",
    "#IN\t   Preposition or conjunction\tin, on, at, since, because\n",
    "#JJ\t   Adjective\t                big, blue, fast\n",
    "#NN\t   Noun, singular or mass\t    car, city, language\n",
    "#VBZ   Verb,present                 runs, is, goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c5fec3a-20d8-44ee-b478-c19d2eede82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ecc3852a-eee3-4268-a119-355266632b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sachin', 'GOAT', 'previous', 'generation', '.', 'Virat', 'GOAT', 'generation', '.', 'Shubman', 'GOAT', 'next', 'generation']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(corpus)\n",
    "cleaned_tokens = []\n",
    "for token in tokens:\n",
    "  if (token not in stop_words):\n",
    "    cleaned_tokens.append(token)\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84c8baf1-9f0c-4a4e-adbd-49d99cb742bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sachin', 'goat', 'previou', 'gener', '.', 'virat', 'goat', 'gener', '.', 'shubman', 'goat', 'next', 'gener']\n"
     ]
    }
   ],
   "source": [
    "# Stemming[Stemming is the process of reducing a word to its root form by removing suffixes. It helps in normalizing words so that \n",
    "#different variations of a word (e.g., \"running,\" \"runs,\" \"ran\") are reduced to a common base form (e.g., \"run\").]\n",
    "from nltk.stem import PorterStemmer # Importing the PorterStemmer class\n",
    "stemmer = PorterStemmer() # Creating an instance of PorterStemmer\n",
    "stemmed_tokens = []\n",
    "for token in cleaned_tokens:\n",
    "  stemmed = stemmer.stem(token) # Applying stemming to each token\n",
    "  stemmed_tokens.append(stemmed) # Adding the stemmed word to the list\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15c7db-28c2-4d5f-b708-33a5b9def240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb20f99-d075-46b2-81b2-2475c95b426d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abd49f3d-fcf0-408b-9be1-bc6cea7f4963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sachin', 'GOAT', 'previous', 'generation', '.', 'Virat', 'GOAT', 'generation', '.', 'Shubman', 'GOAT', 'next', 'generation']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization[Lemmatization is the process of reducing a word to its base or dictionary form (lemma) while ensuring\n",
    "#that the result is a valid word. Unlike stemming, lemmatization considers context and part of speech (POS) to provide more accurate base forms.]\n",
    "# eg studies become studi in stemming while it become study in lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = []\n",
    "for token in cleaned_tokens:\n",
    "  lemmatized = lemmatizer.lemmatize(token)\n",
    "  lemmatized_tokens.append(lemmatized)\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "503fa207-4c7f-471c-a76e-49807d434f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "# TERM FREQUENCY\n",
    "#Term Frequency (TF) is a measure of how often a word appears in a document compared to the total number of words in that document.\n",
    "# Calculate TF for \"cat\" in this document: \"The cat sat on the mat. The cat is happy.\"=>2/10\n",
    "def compute_tf(word, document):\n",
    "    words = document.lower().split()  # Convert to lowercase and split into words\n",
    "    word_count = words.count(word)  # Count occurrences of the word\n",
    "    total_words = len(words)  # Total number of words in the document\n",
    "    return word_count / total_words \n",
    "\n",
    "# Example usage\n",
    "document = \"The cat sat on the mat. The cat is happy.\"\n",
    "word = \"cat\"\n",
    "tf = compute_tf(word, document)\n",
    "print(tf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f35c0ec-83b3-496a-aa99-ffdbce1aaf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4054651081081644\n"
     ]
    }
   ],
   "source": [
    "#inverse document frquency\n",
    "#idf=ln(total number of document/no of document containing that word)\n",
    "import math\n",
    "\n",
    "docs = [\"cat is here\", \"dog is here\", \"cat and dog\"]\n",
    "word = \"cat\"\n",
    "\n",
    "count = sum(word in doc for doc in docs)  \n",
    "idf = math.log(len(docs)/count)  \n",
    "print(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b120e7e-eff6-416d-b1df-bc29705779c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c91aa6-39b5-41eb-9348-5824ffcd6326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23929238-0103-4127-9a26-ba6dfa829056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1839e4b-72c2-4791-9b0c-2b0eb342df68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a95ef-c748-4f48-8698-8f8870cb2929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eceb11-8f38-4d50-87eb-65af1baaf434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
